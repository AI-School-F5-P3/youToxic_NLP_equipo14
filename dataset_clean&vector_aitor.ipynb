{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar y Examinar el dataset\n",
    "Importar pandas, cargar el dataframe y examinar las características de la variable 'Text' mediante descripción estaística básica y una muestra.\n",
    "\n",
    "Conservar solo dos columnas ['Text' & 'IsToxic'] por simplicidad del modelo, eficiencia computacional, reducir la introducción de ruido, evitar el sobreajuste y mejorar su interpretabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "                                                Text  IsToxic\n",
      "0  If only people would just take a step back and...    False\n",
      "1  Law enforcement is not trained to shoot to app...     True\n",
      "2  \\nDont you reckon them 'black lives matter' ba...     True\n",
      "3  There are a very large number of people who do...    False\n",
      "4  The Arab dude is absolutely right, he should h...    False\n",
      "Text       object\n",
      "IsToxic      bool\n",
      "dtype: object\n",
      "==================================================\n",
      "Analisis descriptivo de la variable Text: \n",
      "count              1000\n",
      "unique              997\n",
      "top       run them over\n",
      "freq                  3\n",
      "Name: Text, dtype: object\n",
      "==================================================\n",
      "Muestra aleatoria de la variable Text: \n",
      "428    THANK YOU! right on.black people are going to ...\n",
      "512    When did coming towards the officer is the sam...\n",
      "107    This is a genuine failure of leadership by the...\n",
      "47     This moron is talking about being peaceful?  T...\n",
      "625    They just need to arrest all those protesters ...\n",
      "942    i like how blacks can be all racist against wh...\n",
      "720    I want to toss one of those gernades lol not a...\n",
      "369    It is better for society, for the \"greater goo...\n",
      "330                                Pants up, don't loot!\n",
      "59     Racial profiling and police intimidation? He r...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Importar librería para cargar el dataset y extraer columnas\n",
    "%pip install -q pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# cargar el dataset desde la carpeta Data\n",
    "data = pd.read_csv('Data/youtoxic_english_1000.csv')\n",
    "df = data[['Text', 'IsToxic']]\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Analisis descriptivo de la variable Text: \\n{df['Text'].describe()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Muestra aleatoria de la variable Text: \\n{df['Text'].sample(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores duplicados en la columna Text: \n",
      "              Text  IsToxic\n",
      "592  RUN THEM OVER     True\n",
      "642  run them over     True\n",
      "657  run them over     True\n",
      "677  run them over     True\n",
      "699  RUN THEM OVER     True\n"
     ]
    }
   ],
   "source": [
    "# Encontrar valores repetidos en la columna Text\n",
    "duplicates = df[df.duplicated(['Text'], keep=False)]\n",
    "print(f\"Valores duplicados en la variable Text: \\n{duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiar 'Text' de Caracteres Especiales \n",
    "Uso de expresiones regulares y operaciones con strings para eliminar o reemplazar caracteres especiales, URLs y símbolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767    As a black person all I have to say is THANK Y...\n",
       "677                                        run them over\n",
       "419    I cant tell if this is a racist cop or a dumb ...\n",
       "119            The only good CNN anchor I have ever seen\n",
       "734    finally a black person with a unbiased opinion...\n",
       "410    Whether youre right or wrong about the Michael...\n",
       "63     This Muslim man does not have anything in mind...\n",
       "831    Well said Peggy your message should be shared ...\n",
       "595                                       alllivesmatter\n",
       "836    Yea We should beat up all white people rob bla...\n",
       "Name: Cleaned_Text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar “handles” de Twitter, hashtags y números\n",
    "    text = re.sub(r'\\@\\w+|\\#|\\d+', '', text)\n",
    "    # Eliminar signos de puntuación\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Aplicar la función clean_text a la columna Text\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Muestra aleatoria de la columna Cleaned_Text\n",
    "df['Cleaned_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de 'Text'\n",
    "Convierte texto a minúsculas, elimina acentos y normaliza espacios en blanco. Maneja abreviaturas y contracciones comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992    there are a lot of disgusting people in the co...\n",
       "241                            confederation ill be back\n",
       "558    the cop had a fractured eye socket and the fis...\n",
       "810    this older white conservative says peggy for v...\n",
       "955    they cant help it but these motherfuckers are ...\n",
       "159    why the hell are they even interviewing him th...\n",
       "698    its hilarious seeing these morons get hit by cars\n",
       "367    your gentle giant seems to have robbed a store...\n",
       "879    the cops need to look at the video and start a...\n",
       "935                                 the lady at hahahaha\n",
       "Name: Normalized_Text, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convierte el texto a minúsculas\n",
    "    text = text.lower()\n",
    "    # Elimina los acentos\n",
    "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    # Normaliza los espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Aplica la función normalize_text a la columna Cleaned_Text\n",
    "df['Normalized_Text'] = df['Cleaned_Text'].apply(normalize_text)\n",
    "\n",
    "# Muestra aleatoria de la columna Normalized_Text\n",
    "df['Normalized_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar Stopwords\n",
    "Importar NLTK y descargar stopwords para el idioma Inglés.\n",
    "Las stopwords se utilizan comúnmente en la minería de texto y el procesamiento del lenguaje natural para eliminar palabras que se usan tan ampliamente que contienen muy poca información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/aitor/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "853                            bunch complete dumb asses\n",
       "167               guy idiot thats best lost already bozo\n",
       "572                         would eating coal could roll\n",
       "499                                      good one stefan\n",
       "143    wow think im going buy bullet proof vestshit b...\n",
       "22                                      word provocateur\n",
       "4      arab dude absolutely right shot extra time sho...\n",
       "932    put civilians perspective two people get fight...\n",
       "725                  peggy im native american love honey\n",
       "193            people fuck america american fucking hate\n",
       "Name: Processed_Text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Separar el texto en palabras\n",
    "    words = text.split()\n",
    "    # Eliminar stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Unir las palabras nuevamente en un solo string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Aplica la función remove_stopwords a la columna Normalized_Text\n",
    "df['Processed_Text'] = df['Normalized_Text'].apply(remove_stopwords)\n",
    "\n",
    "# Muestra aleatoria de la columna Processed_Text\n",
    "df['Processed_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313                  informative really appreciate video\n",
       "649                  someone taken one team plowed right\n",
       "233    im point think cop points gun know im told som...\n",
       "338    take medical cannabis guy liked couple face bo...\n",
       "544    need able call people guns ca citizens complet...\n",
       "43      smirconnish evenhanded guy world guy masri idiot\n",
       "752                                           peggy hero\n",
       "804    amen peggy hubbard color makes difference pers...\n",
       "776    need people like peggy hubbard support mother ...\n",
       "497    picture cops injuries wasnt ambulance injured ...\n",
       "Name: NoEmoji_Processed_Text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_emoji(text):\n",
    "    \n",
    "    # Eliminar emojis\n",
    "    text = re.sub(r'[^\\u0000-\\u007F]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Aplica la función clean_emoji a la columna Processed_Text\n",
    "df['NoEmoji_Processed_Text'] = df['Processed_Text'].apply(clean_emoji)\n",
    "\n",
    "# Muestra aleatoria de la columna NoEmoji_Processed_Text\n",
    "df['NoEmoji_Processed_Text'].sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
