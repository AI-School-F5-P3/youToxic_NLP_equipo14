{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar y Examinar el dataset\n",
    "Importar pandas, cargar el dataframe y examinar las caracter√≠sticas de la variable 'Text' mediante descripci√≥n esta√≠stica b√°sica y una muestra.\n",
    "\n",
    "Conservar solo dos columnas ['Text' & 'IsToxic'] por simplicidad del modelo, eficiencia computacional, reducir la introducci√≥n de ruido, evitar el sobreajuste y mejorar su interpretabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "                                                Text  IsToxic\n",
      "0  If only people would just take a step back and...    False\n",
      "1  Law enforcement is not trained to shoot to app...     True\n",
      "2  \\nDont you reckon them 'black lives matter' ba...     True\n",
      "3  There are a very large number of people who do...    False\n",
      "4  The Arab dude is absolutely right, he should h...    False\n",
      "Text       object\n",
      "IsToxic      bool\n",
      "dtype: object\n",
      "==================================================\n",
      "Analisis descriptivo de la variable Text: \n",
      "count              1000\n",
      "unique              997\n",
      "top       run them over\n",
      "freq                  3\n",
      "Name: Text, dtype: object\n",
      "==================================================\n",
      "Muestra aleatoria de la variable Text: \n",
      "164    Bassem Masri is an uneducated idiot. Just list...\n",
      "901                          7:24 7:30 XDDDDDDDDDDDDDDDD\n",
      "133    I made a song addressing Ferguson and the issu...\n",
      "236             to bad those weapons were not discharged\n",
      "522    \"Almost floor shaving quality cigars...\"\\n\\n\\n...\n",
      "394    moly loooooooooooooooooooooooves cops.¬† there ...\n",
      "470    Black cop kills white kid...Black kid kills wh...\n",
      "69                      \"you know what I'm sayin\"... wow\n",
      "518    It's amazing the ONLY info on the police offic...\n",
      "339    LOL, trayvon's online name was \"nolimitnigga\"....\n",
      "Name: Text, dtype: object\n",
      "==================================================\n",
      "Analisis descriptivo de la variable IsToxic: \n",
      "count      1000\n",
      "unique        2\n",
      "top       False\n",
      "freq        538\n",
      "Name: IsToxic, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Importar pandas para cargar el dataset y extraer columnas\n",
    "%pip install -q pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# cargar el dataset desde la carpeta Data\n",
    "data = pd.read_csv('Data/youtoxic_english_1000.csv')\n",
    "df = data[['Text', 'IsToxic']]\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Analisis descriptivo de la variable Text: \\n{df['Text'].describe()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Muestra aleatoria de la variable Text: \\n{df['Text'].sample(10)}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Analisis descriptivo de la variable IsToxic: \\n{df['IsToxic'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores duplicados en la variable Text: \n",
      "              Text  IsToxic\n",
      "592  RUN THEM OVER     True\n",
      "642  run them over     True\n",
      "657  run them over     True\n",
      "677  run them over     True\n",
      "699  RUN THEM OVER     True\n"
     ]
    }
   ],
   "source": [
    "# Encontrar valores repetidos en la columna Text\n",
    "duplicates = df[df.duplicated(['Text'], keep=False)]\n",
    "print(f\"Valores duplicados en la variable Text: \\n{duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiar 'Text' de Caracteres Especiales \n",
    "Uso de expresiones regulares y operaciones con strings para eliminar o reemplazar caracteres especiales, URLs y s√≠mbolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884        In the point they almost set a person on fire\n",
       "759    So proud of a woman  mother that stands on pri...\n",
       "259    You know its crazy I actually came across your...\n",
       "74                                  plain n simple truth\n",
       "772                        PEGGY HUBBARD for PRESIDENT  \n",
       "401                As always Mr Molyneux excellent video\n",
       "465    ¬†Kajieme Powell Shooting Video Disturbs For So...\n",
       "238    Wait so who was rioting again The people stand...\n",
       "315    Great video Levelheaded cogent and fair Unfort...\n",
       "621                                 All liberal nonsense\n",
       "Name: Cleaned_Text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar ‚Äúhandles‚Äù de Twitter, hashtags y n√∫meros\n",
    "    text = re.sub(r'\\@\\w+|\\#|\\d+', '', text)\n",
    "    # Eliminar signos de puntuaci√≥n\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Aplicar la funci√≥n clean_text a la columna Text\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Muestra aleatoria de la columna Cleaned_Text\n",
    "df['Cleaned_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizaci√≥n de 'Text'\n",
    "Convierte texto a min√∫sculas, elimina acentos y normaliza espacios en blanco. Maneja abreviaturas y contracciones comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828    god bless this lady i wish there were more peo...\n",
       "362    my god look at this raw footage shocking fergu...\n",
       "415    as a white guy if you are really concerned wit...\n",
       "421    apparently stefan never read whats the matter ...\n",
       "343    mr stefan molyneux all mr wilson had to do was...\n",
       "767    as a black person all i have to say is thank y...\n",
       "138    if that cop shot a white guy there would not b...\n",
       "463    i am not against most of what is being present...\n",
       "412    link between poor little tryvon martin and lit...\n",
       "160    oh my god i fucking love that arab guy such wi...\n",
       "Name: Normalized_Text, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convierte el texto a min√∫sculas\n",
    "    text = text.lower()\n",
    "    # Normaliza los espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Normailza las contrracciones comunes en ingl√©s\n",
    "    text = text.replace(\"what's\", \"what is \")\n",
    "    text = text.replace(\"'s\", \" \")\n",
    "    text = text.replace(\"'ve\", \" have \")\n",
    "    text = text.replace(\"can't\", \"cannot \")\n",
    "    text = text.replace(\"n't\", \" not \")\n",
    "    text = text.replace(\"i'm\", \"i am \")\n",
    "    text = text.replace(\"'re\", \" are \")\n",
    "    text = text.replace(\"'d\", \" would \")\n",
    "    text = text.replace(\"'ll\", \" will \")\n",
    "    # Elimina los acentos\n",
    "    # text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    return text\n",
    "\n",
    "# Aplica la funci√≥n normalize_text a la columna Cleaned_Text\n",
    "df['Normalized_Text'] = df['Cleaned_Text'].apply(normalize_text)\n",
    "\n",
    "# Muestra aleatoria de la columna Normalized_Text\n",
    "df['Normalized_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar Stopwords\n",
    "Importar NLTK y descargar stopwords para el idioma Ingl√©s.\n",
    "Las stopwords se utilizan com√∫nmente en la miner√≠a de texto y el procesamiento del lenguaje natural para eliminar palabras que se usan tan ampliamente que contienen muy poca informaci√≥n √∫til."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/aitor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "126    trayvon martin michael brown common real reaso...\n",
       "888    truly tragedynot death kid way city reacted wa...\n",
       "388    well know truth today yet still hear hands don...\n",
       "175    people say blacks play victim according commen...\n",
       "846    real criminals wall street guys steal money in...\n",
       "766                                          agree sista\n",
       "390    everyone knows white people die cops blacks ev...\n",
       "975    americas days coming end emigration rate multi...\n",
       "797                                       right dynamite\n",
       "378    well ya go young white man killed nonwhite say...\n",
       "Name: NoStopWords_Text, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Separar el texto en palabras\n",
    "    words = text.split()\n",
    "    # Eliminar stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Unir las palabras nuevamente en un solo string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Aplica la funci√≥n remove_stopwords a la columna Normalized_Text\n",
    "df['NoStopWords_Text'] = df['Normalized_Text'].apply(remove_stopwords)\n",
    "\n",
    "# Muestra aleatoria de la columna Processed_Text\n",
    "df['NoStopWords_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar Emojis üòµ‚Äçüí´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311    michael brown positively identified assailant ...\n",
       "484                                shoot someone run lol\n",
       "453    music references sex drugs stealing murder etc...\n",
       "376    youtube comwatchvyfbbtdvhg research didnt come...\n",
       "544    need able call people guns ca citizens complet...\n",
       "119                            good cnn anchor ever seen\n",
       "257    funny thing government trusted judge fucking d...\n",
       "732     love woman voice matters fuck black lives matter\n",
       "487     advertising ignorance creates much crime opinion\n",
       "854    ignorance law excuse usc deprivation rights co...\n",
       "Name: NoEmoji_Processed_Text, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_emoji(text):\n",
    "    \n",
    "    # Eliminar emojis\n",
    "    text = re.sub(r'[^\\u0000-\\u007F]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Aplica la funci√≥n clean_emoji a la columna Processed_Text\n",
    "df['NoEmoji_Processed_Text'] = df['NoStopWords_Text'].apply(clean_emoji)\n",
    "\n",
    "# Muestra aleatoria de la columna NoEmoji_Processed_Text\n",
    "df['NoEmoji_Processed_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformar el texto con m√©todo TF-IDF\n",
    "Transformar el texto procesado en una representaci√≥n num√©rica utilizando el m√©todo TF-IDF (Term Frequency-Inverse Document Frequency), considerando n-gramas y otras caracter√≠sticas de 'NoEmoji_Precessed_Text'. El resultado ser√° un dataframe de vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5000)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaannnyything</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely nothing</th>\n",
       "      <th>abuse</th>\n",
       "      <th>according</th>\n",
       "      <th>accountable</th>\n",
       "      <th>accountable actions</th>\n",
       "      <th>accounts</th>\n",
       "      <th>...</th>\n",
       "      <th>young white</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zimmerman case</th>\n",
       "      <th>zimmerman michael</th>\n",
       "      <th>zimmermans</th>\n",
       "      <th>zionist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aaannnyything  ability  able  absolutely  absolutely nothing  abuse  \\\n",
       "688            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "491            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "854            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "138            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "673            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "111            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "648            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "957            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "483            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "153            0.0      0.0   0.0         0.0                 0.0    0.0   \n",
       "\n",
       "     according  accountable  accountable actions  accounts  ...  young white  \\\n",
       "688        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "491        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "854        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "138        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "673        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "111        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "648        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "957        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "483        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "153        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "\n",
       "     youre  youth  youtube  youve  zimmerman  zimmerman case  \\\n",
       "688    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "491    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "854    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "138    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "673    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "111    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "648    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "957    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "483    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "153    0.0    0.0      0.0    0.0        0.0             0.0   \n",
       "\n",
       "     zimmerman michael  zimmermans  zionist  \n",
       "688                0.0         0.0      0.0  \n",
       "491                0.0         0.0      0.0  \n",
       "854                0.0         0.0      0.0  \n",
       "138                0.0         0.0      0.0  \n",
       "673                0.0         0.0      0.0  \n",
       "111                0.0         0.0      0.0  \n",
       "648                0.0         0.0      0.0  \n",
       "957                0.0         0.0      0.0  \n",
       "483                0.0         0.0      0.0  \n",
       "153                0.0         0.0      0.0  \n",
       "\n",
       "[10 rows x 5000 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Iniciar el TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "# Aprender y transformar\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['NoEmoji_Processed_Text'])\n",
    "\n",
    "# Convertir la matriz en un DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Mostrar la forma del DataFrame\n",
    "print(tfidf_df.shape)\n",
    "print(\"=\"*50)\n",
    "# Visualizar una muestra aleatoria\n",
    "tfidf_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinar y guardar el procesado como un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar el dataframe 'tfidf' con las etiquetas 'IsToxic'\n",
    "features = tfidf_df\n",
    "labels = df['IsToxic']\n",
    "\n",
    "processed_df = pd.concat([features, labels], axis=1)\n",
    "\n",
    "# Guardar el dataframe procesado en un archivo CSV dentro de la carpeta Data\n",
    "processed_df.to_csv('Data/youtoxic_english_1000_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
