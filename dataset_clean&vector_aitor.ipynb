{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar y Examinar el dataset\n",
    "Importar pandas, cargar el dataframe y examinar las características de la variable 'Text' mediante descripción estaística básica y una muestra.\n",
    "\n",
    "Conservar solo dos columnas ['Text' & 'IsToxic'] por simplicidad del modelo, eficiencia computacional, reducir la introducción de ruido, evitar el sobreajuste y mejorar su interpretabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "                                                Text  IsToxic\n",
      "0  If only people would just take a step back and...    False\n",
      "1  Law enforcement is not trained to shoot to app...     True\n",
      "2  \\nDont you reckon them 'black lives matter' ba...     True\n",
      "3  There are a very large number of people who do...    False\n",
      "4  The Arab dude is absolutely right, he should h...    False\n",
      "Text       object\n",
      "IsToxic      bool\n",
      "dtype: object\n",
      "==================================================\n",
      "Analisis descriptivo de la variable Text: \n",
      "count              1000\n",
      "unique              997\n",
      "top       run them over\n",
      "freq                  3\n",
      "Name: Text, dtype: object\n",
      "==================================================\n",
      "Muestra aleatoria de la variable Text: \n",
      "238    Wait, so who was rioting again? The people sta...\n",
      "79      That motherfucker was angry,and I understand why\n",
      "964    So I guess he gets away with it since he had a...\n",
      "344    For all you people saying the cop was justifie...\n",
      "52     CNN is propaganda for the most part, and peopl...\n",
      "714    I'm not informed 100% about what exactly is go...\n",
      "371            awnser to 24:15 question aim for the leg \n",
      "657                                        run them over\n",
      "918                              Fuck mo and kansas city\n",
      "102    \"I don't want my sons death to be in vain.\" \"L...\n",
      "Name: Text, dtype: object\n",
      "==================================================\n",
      "Analisis descriptivo de la variable IsToxic: \n",
      "count      1000\n",
      "unique        2\n",
      "top       False\n",
      "freq        538\n",
      "Name: IsToxic, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Importar pandas para cargar el dataset y extraer columnas\n",
    "%pip install -q pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# cargar el dataset desde la carpeta Data\n",
    "data = pd.read_csv('Data/youtoxic_english_1000.csv')\n",
    "df = data[['Text', 'IsToxic']]\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Analisis descriptivo de la variable Text: \\n{df['Text'].describe()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Muestra aleatoria de la variable Text: \\n{df['Text'].sample(10)}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Analisis descriptivo de la variable IsToxic: \\n{df['IsToxic'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores duplicados en la variable Text: \n",
      "              Text  IsToxic\n",
      "592  RUN THEM OVER     True\n",
      "642  run them over     True\n",
      "657  run them over     True\n",
      "677  run them over     True\n",
      "699  RUN THEM OVER     True\n"
     ]
    }
   ],
   "source": [
    "# Encontrar valores repetidos en la columna Text\n",
    "duplicates = df[df.duplicated(['Text'], keep=False)]\n",
    "print(f\"Valores duplicados en la variable Text: \\n{duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiar 'Text' de Caracteres Especiales \n",
    "Uso de expresiones regulares y operaciones con strings para eliminar o reemplazar caracteres especiales, URLs y símbolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/sr4mx6rx4qn0h7370bd1mny00000gn/T/ipykernel_68558/1669119510.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Cleaned_Text'] = df['Text'].apply(clean_text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "258    What is the relevance of marijuana here\\n\\nThe...\n",
       "837                                 blacklivesdontmatter\n",
       "895    Some people have seen Mississippi Burning way ...\n",
       "355     agree people are up in arms but they need to ...\n",
       "51     The world is changing im mad at the police too...\n",
       "808                  No Leave it alone Dont take it down\n",
       "978             I like how they say hurr instead of here\n",
       "97                                   This guy is a moron\n",
       "256    What does this video have to do with what actu...\n",
       "781    Dear Peggy can you please be all of our Mother...\n",
       "Name: Cleaned_Text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar “handles” de Twitter, hashtags y números\n",
    "    text = re.sub(r'\\@\\w+|\\#|\\d+', '', text)\n",
    "    # Eliminar signos de puntuación\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Aplicar la función clean_text a la columna Text\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Muestra aleatoria de la columna Cleaned_Text\n",
    "df['Cleaned_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de 'Text'\n",
    "Convierte texto a minúsculas, elimina acentos y normaliza espacios en blanco. Maneja abreviaturas y contracciones comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/sr4mx6rx4qn0h7370bd1mny00000gn/T/ipykernel_68558/288376120.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Normalized_Text'] = df['Cleaned_Text'].apply(normalize_text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "414    michale brown got what he was asking for the b...\n",
       "58     can anyone tell me why it is racist if a white...\n",
       "391    can someone please turn that into an autotune ...\n",
       "45     he had to take a break because bassem was maki...\n",
       "532    ana kasparian is the most beautiful woman in t...\n",
       "834             really black people are the most violent\n",
       "193    the more those people fuck up america the more...\n",
       "378    well here ya go young white man killed by a no...\n",
       "11     moral of the story dont reach for a cops gun b...\n",
       "950     why is the cop mooing at people in the beginning\n",
       "Name: Normalized_Text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convierte el texto a minúsculas\n",
    "    text = text.lower()\n",
    "    # Normaliza los espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Normailza las contrracciones comunes en inglés\n",
    "    text = text.replace(\"what's\", \"what is \")\n",
    "    text = text.replace(\"'s\", \" \")\n",
    "    text = text.replace(\"'ve\", \" have \")\n",
    "    text = text.replace(\"can't\", \"cannot \")\n",
    "    text = text.replace(\"n't\", \" not \")\n",
    "    text = text.replace(\"i'm\", \"i am \")\n",
    "    text = text.replace(\"'re\", \" are \")\n",
    "    text = text.replace(\"'d\", \" would \")\n",
    "    text = text.replace(\"'ll\", \" will \")\n",
    "    # Elimina los acentos\n",
    "    # text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    return text\n",
    "\n",
    "# Aplica la función normalize_text a la columna Cleaned_Text\n",
    "df['Normalized_Text'] = df['Cleaned_Text'].apply(normalize_text)\n",
    "\n",
    "# Muestra aleatoria de la columna Normalized_Text\n",
    "df['Normalized_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar Stopwords\n",
    "Importar NLTK y descargar stopwords para el idioma Inglés.\n",
    "Las stopwords se utilizan comúnmente en la minería de texto y el procesamiento del lenguaje natural para eliminar palabras que se usan tan ampliamente que contienen muy poca información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/aitor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/_g/sr4mx6rx4qn0h7370bd1mny00000gn/T/ipykernel_68558/2451760209.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['NoStopWords_Text'] = df['Normalized_Text'].apply(remove_stopwords)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "549                                      joe rogan idiot\n",
       "836    yea beat white people rob black owned stores c...\n",
       "169    muslims like spit white people brought muslims...\n",
       "888    truly tragedynot death kid way city reacted wa...\n",
       "993    whites moveout ferguson lets see nice becomes ...\n",
       "949    saw article many cases went pretty much one pa...\n",
       "560           hey shout thai writing shirt lol thaipride\n",
       "915    dont want justice see mob mentality action hes...\n",
       "787    women get nobel peace prize instead antichrist...\n",
       "315    great video levelheaded cogent fair unfortunat...\n",
       "Name: NoStopWords_Text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Separar el texto en palabras\n",
    "    words = text.split()\n",
    "    # Eliminar stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Unir las palabras nuevamente en un solo string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Aplica la función remove_stopwords a la columna Normalized_Text\n",
    "df['NoStopWords_Text'] = df['Normalized_Text'].apply(remove_stopwords)\n",
    "\n",
    "# Muestra aleatoria de la columna Processed_Text\n",
    "df['NoStopWords_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar Emojis 😵‍💫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/sr4mx6rx4qn0h7370bd1mny00000gn/T/ipykernel_68558/2059466865.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['NoEmoji_Processed_Text'] = df['NoStopWords_Text'].apply(clean_emoji)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "471    video pushing clerk around really made lose sy...\n",
       "867    police shot black guy blacks riot black shoot ...\n",
       "472    easy point negative black man point good point...\n",
       "58     anyone tell racist white man shoots black man ...\n",
       "180    say black piece came bit stiff due repeated us...\n",
       "442    dude someone please tell dude listen violent m...\n",
       "338    take medical cannabis guy liked couple face bo...\n",
       "204    majority people ferguson african american elec...\n",
       "310    major difference zimmerman case one shooter zi...\n",
       "554                   look mike brown world instrumental\n",
       "Name: NoEmoji_Processed_Text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_emoji(text):\n",
    "    \n",
    "    # Eliminar emojis\n",
    "    text = re.sub(r'[^\\u0000-\\u007F]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Aplica la función clean_emoji a la columna Processed_Text\n",
    "df['NoEmoji_Processed_Text'] = df['NoStopWords_Text'].apply(clean_emoji)\n",
    "\n",
    "# Muestra aleatoria de la columna NoEmoji_Processed_Text\n",
    "df['NoEmoji_Processed_Text'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformar el texto con método TF-IDF\n",
    "Transformar el texto procesado en una representación numérica utilizando el método TF-IDF (Term Frequency-Inverse Document Frequency), considerando n-gramas y otras características de 'NoEmoji_Precessed_Text'. El resultado será un dataframe de vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "(1000, 5000)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaannnyything</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely nothing</th>\n",
       "      <th>abuse</th>\n",
       "      <th>according</th>\n",
       "      <th>accountable</th>\n",
       "      <th>accountable actions</th>\n",
       "      <th>accounts</th>\n",
       "      <th>...</th>\n",
       "      <th>young white</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zimmerman case</th>\n",
       "      <th>zimmerman michael</th>\n",
       "      <th>zimmermans</th>\n",
       "      <th>zionist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aaannnyything  ability  able  absolutely  absolutely nothing     abuse  \\\n",
       "256            0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "454            0.0      0.0   0.0         0.0                 0.0  0.181738   \n",
       "625            0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "75             0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "335            0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "50             0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "408            0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "393            0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "65             0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "234            0.0      0.0   0.0         0.0                 0.0  0.000000   \n",
       "\n",
       "     according  accountable  accountable actions  accounts  ...  young white  \\\n",
       "256        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "454        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "625        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "75         0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "335        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "50         0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "408        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "393        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "65         0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "234        0.0          0.0                  0.0       0.0  ...          0.0   \n",
       "\n",
       "        youre  youth  youtube  youve  zimmerman  zimmerman case  \\\n",
       "256  0.089258    0.0      0.0    0.0        0.0             0.0   \n",
       "454  0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "625  0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "75   0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "335  0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "50   0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "408  0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "393  0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "65   0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "234  0.000000    0.0      0.0    0.0        0.0             0.0   \n",
       "\n",
       "     zimmerman michael  zimmermans  zionist  \n",
       "256                0.0         0.0      0.0  \n",
       "454                0.0         0.0      0.0  \n",
       "625                0.0         0.0      0.0  \n",
       "75                 0.0         0.0      0.0  \n",
       "335                0.0         0.0      0.0  \n",
       "50                 0.0         0.0      0.0  \n",
       "408                0.0         0.0      0.0  \n",
       "393                0.0         0.0      0.0  \n",
       "65                 0.0         0.0      0.0  \n",
       "234                0.0         0.0      0.0  \n",
       "\n",
       "[10 rows x 5000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Iniciar el TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "# Aprender y transformar\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['NoEmoji_Processed_Text'])\n",
    "\n",
    "# Convertir la matriz en un DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Mostrar la forma del DataFrame\n",
    "print(tfidf_df.shape)\n",
    "print(\"=\"*50)\n",
    "# Visualizar una muestra aleatoria\n",
    "tfidf_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinar y guardar el procesado como un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar el dataframe 'tfidf' con las etiquetas 'IsToxic'\n",
    "features = tfidf_df\n",
    "labels = df['IsToxic']\n",
    "\n",
    "processed_df = pd.concat([features, labels], axis=1)\n",
    "\n",
    "# Guardar el dataframe procesado en un archivo CSV dentro de la carpeta Data\n",
    "processed_df.to_csv('Data/youtoxic_english_1000_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
